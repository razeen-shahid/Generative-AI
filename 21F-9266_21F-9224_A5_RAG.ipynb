{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11244402,"sourceType":"datasetVersion","datasetId":7025667}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing dependencies","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu langchain sentence-transformers\n!pip install -U langchain-community\n!pip install --upgrade huggingface_hub\n!pip install --upgrade sentence-transformers\n!pip install --upgrade bitsandbytes\n!pip install --upgrade transformers accelerate\n!pip install --upgrade transformers bitsandbytes accelerate sentence-transformers\n!pip uninstall langchain langchain-core pydantic -y\n!pip install langchain==0.1.14 langchain-core==0.1.37 pydantic==1.10.13\n!pip install langchain-openai==0.0.8\n!pip show langchain\n!pip show langchain-core\n!pip show pydantic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Knowledge Graphs (diagnostic_kg) → JSON files containing diagnostic pathways and medical knowledge.\n","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nkg_path = \"/kaggle/input/a5-data/mimic-iv-ext-direct-1.0.0/diagnostic_kg/Diagnosis_flowchart\"\nknowledge_graphs = {}\nfor file in os.listdir(kg_path):\n    if file.endswith(\".json\"):\n        with open(os.path.join(kg_path, file), \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        diagnostic_steps = data.get(\"diagnostic\", {})\n        knowledge_info = data.get(\"knowledge\", {})\n        knowledge_text = \"\"\n        for step, details in knowledge_info.items():\n            if isinstance(details, dict):  \n                for key, value in details.items():\n                    knowledge_text += f\"{step} - {key}: {value}\\n\"\n            else:  \n                knowledge_text += f\"{step}: {details}\\n\"\n        knowledge_graphs[file] = knowledge_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:18:24.999155Z","iopub.execute_input":"2025-04-15T19:18:24.999466Z","iopub.status.idle":"2025-04-15T19:18:25.081417Z","shell.execute_reply.started":"2025-04-15T19:18:24.999443Z","shell.execute_reply":"2025-04-15T19:18:25.080571Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Annotated Clinical Notes (samples) → JSON files with real patient records and step-by-step diagnoses.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\n# Path to clinical notes\nsample_path = \"/kaggle/input/a5-data/mimic-iv-ext-direct-1.0.0/samples/Finished\"\n\n# Dictionary to store clinical notes\nclinical_notes = {}\n\n# Loop through disease categories inside 'Finished'\nfor disease_category in os.listdir(sample_path):\n    disease_path = os.path.join(sample_path, disease_category)\n    \n    if os.path.isdir(disease_path):  # Ensure it's a folder\n        clinical_notes[disease_category] = {}\n        \n        # Loop through subcategories inside each disease category\n        for subcategory in os.listdir(disease_path):\n            subcategory_path = os.path.join(disease_path, subcategory)\n            \n            if os.path.isdir(subcategory_path):  # Ensure it's a folder\n                clinical_notes[disease_category][subcategory] = []\n                \n                # Traverse JSON files inside subcategory\n                for file in os.listdir(subcategory_path):\n                    if file.endswith(\".json\"):\n                        file_path = os.path.join(subcategory_path, file)\n                        \n                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                            data = json.load(f)\n                        \n                        # Extract useful fields\n                        note_text = \"\"\n                        for key, value in data.items():\n                            note_text += f\"{key}: {value}\\n\"\n                        \n                        # Store extracted text\n                        clinical_notes[disease_category][subcategory].append(note_text)\n\n# Now 'clinical_notes' contains structured data organized by disease category and subcategory\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:18:28.434817Z","iopub.execute_input":"2025-04-15T19:18:28.435110Z","iopub.status.idle":"2025-04-15T19:18:29.793695Z","shell.execute_reply.started":"2025-04-15T19:18:28.435089Z","shell.execute_reply":"2025-04-15T19:18:29.792986Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Cleaning Text","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    \"\"\"\n    Cleans and normalizes medical text.\n    - Removes special characters\n    - Normalizes spacing\n    - Converts lists into readable format\n    \"\"\"\n    text = re.sub(r'\\n+', '\\n', text)  # Remove extra new lines\n    text = re.sub(r'[^\\w\\s.;,]', '', text)  # Remove special characters\n    text = text.strip()\n    return text\n\ndef clean_nested_data(data):\n    \"\"\"\n    Recursively applies cleaning to nested data (text content).\n    \"\"\"\n    if isinstance(data, str):  # If the data is a string, clean it\n        return clean_text(data)\n    \n    if isinstance(data, dict):  # If the data is a dictionary, apply cleaning recursively\n        return {key: clean_nested_data(value) for key, value in data.items()}\n    \n    if isinstance(data, list):  # If the data is a list, apply cleaning recursively to each item\n        return [clean_nested_data(item) for item in data]\n    \n    return data  # If it's neither string, dict, nor list, return as is\n\n# Apply cleaning to both knowledge graphs and clinical notes\nknowledge_graphs = {key: clean_text(value) for key, value in knowledge_graphs.items()}\nclinical_notes = clean_nested_data(clinical_notes)  # Apply the recursive cleaning for nested structure\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:19:14.068559Z","iopub.execute_input":"2025-04-15T19:19:14.068901Z","iopub.status.idle":"2025-04-15T19:19:14.162825Z","shell.execute_reply.started":"2025-04-15T19:19:14.068876Z","shell.execute_reply":"2025-04-15T19:19:14.162104Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Setting Format for Data Retrieval","metadata":{}},{"cell_type":"code","source":"from langchain.docstore.document import Document\n\n# Prepare documents for retrieval\ndocuments = []\n\n# Convert Knowledge Graphs\nfor file, content in knowledge_graphs.items():\n    documents.append(Document(page_content=content, metadata={\"source\": file, \"type\": \"knowledge_graph\"}))\n\n# Convert Clinical Notes (Handling Nested Structure)\nfor disease_category, subcategories in clinical_notes.items():\n    for subcategory, notes in subcategories.items():\n        for idx, note in enumerate(notes):  # Each note is a separate document\n            documents.append(Document(\n                page_content=note,\n                metadata={\n                    \"source\": f\"{disease_category}/{subcategory}/note_{idx+1}\",\n                    \"type\": \"clinical_note\",\n                    \"disease_category\": disease_category,\n                    \"subcategory\": subcategory\n                }\n            ))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:20:30.503612Z","iopub.execute_input":"2025-04-15T19:20:30.503912Z","iopub.status.idle":"2025-04-15T19:20:31.863301Z","shell.execute_reply.started":"2025-04-15T19:20:30.503890Z","shell.execute_reply":"2025-04-15T19:20:31.862606Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Storing Data in Vector form","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom sentence_transformers import SentenceTransformer\n\n# Verify CUDA and GPU\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n\n# Configure 4-bit quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\n# Load the model manually with quantization\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n\n# Wrap it into SentenceTransformer\nembedding_model = SentenceTransformer(model_name)\nembedding_model._first_module().auto_model = model  # Inject the quantized model\n\n# Initialize embeddings\nembedding = HuggingFaceEmbeddings(model_name=model_name)\n\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:27:41.542629Z","iopub.execute_input":"2025-04-15T19:27:41.543085Z","iopub.status.idle":"2025-04-15T19:28:17.840875Z","shell.execute_reply.started":"2025-04-15T19:27:41.543054Z","shell.execute_reply":"2025-04-15T19:28:17.840075Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU Device: Tesla T4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec067a5ebf7b450cba202a72c0d38e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf90e19a15844d67950e235647339208"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26bcedcc90ab4d0bb3abf7ab2d26a2a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"825a2cdc2c96495abfcd9b318a1c8a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2030446fd34b7fa4fb3eb34454e803"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6d9f9a2047400b8f55664310aeb77f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f076e9fb8854df0ab75f83051129de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ad468e5d6c47e086b9d145ecb5ba38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266584eb02c247c4883c63ab2935964b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8abd0bea784fce9197cd0038b177a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7a0dfe82634a81a295d2132ad9a285"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-9-0278ce448c85>:28: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embedding = HuggingFaceEmbeddings(model_name=model_name)\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Converting Documents in Embeddings","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore.document import Document\n\n# Initialize the embedding model with BERT base uncased\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # 384-dim model\n\n# Convert dictionaries to LangChain Documents\ndocuments = []\n\n# Knowledge Graphs\nfor file, content in knowledge_graphs.items():\n    documents.append(Document(page_content=content, metadata={\"source\": file, \"type\": \"knowledge_graph\"}))\n\n# Clinical Notes\nfor disease, subcategories in clinical_notes.items():\n    for subcategory, notes in subcategories.items():\n        for note in notes:\n            documents.append(Document(page_content=note, metadata={\"source\": disease, \"subcategory\": subcategory, \"type\": \"clinical_note\"}))\nfaiss_index = FAISS.from_documents(documents, embedding_model)\nfaiss_index.save_local(\"faiss_index\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:29:11.231982Z","iopub.execute_input":"2025-04-15T19:29:11.232308Z","iopub.status.idle":"2025-04-15T19:29:13.868689Z","shell.execute_reply.started":"2025-04-15T19:29:11.232286Z","shell.execute_reply":"2025-04-15T19:29:13.867597Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"vectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:29:20.957674Z","iopub.execute_input":"2025-04-15T19:29:20.958051Z","iopub.status.idle":"2025-04-15T19:29:20.969259Z","shell.execute_reply.started":"2025-04-15T19:29:20.958027Z","shell.execute_reply":"2025-04-15T19:29:20.967548Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Using Top-K for results","metadata":{}},{"cell_type":"markdown","source":"# Load the FAISS Vector Store","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:35:43.852255Z","iopub.execute_input":"2025-04-15T19:35:43.852574Z","iopub.status.idle":"2025-04-15T19:35:43.931251Z","shell.execute_reply.started":"2025-04-15T19:35:43.852550Z","shell.execute_reply":"2025-04-15T19:35:43.930403Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom transformers import pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_openai import ChatOpenAI\nllm = ChatOpenAI()\nfrom langchain.chains import RetrievalQA\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom transformers import pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:48:06.627407Z","iopub.execute_input":"2025-04-15T19:48:06.627873Z","iopub.status.idle":"2025-04-15T19:48:06.777932Z","shell.execute_reply.started":"2025-04-15T19:48:06.627835Z","shell.execute_reply":"2025-04-15T19:48:06.776274Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPydanticImportError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-c45d5ac28232>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from langchain_openai.chat_models import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mAzureChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m from langchain_openai.embeddings import (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mazure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"ChatOpenAI\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/azure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSecretStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_validator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotGiven\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProxiesTypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfile_from_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/types/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/types/batch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch_error\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch_request_counts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchRequestCounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFieldInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/generics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mExtLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_validators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgather_all_validators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeferredType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/class_validators.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36minit pydantic.class_validators\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(name)\u001b[0m\n","\u001b[0;31mPydanticImportError\u001b[0m: `pydantic.errors:ConfigError` has been removed in V2.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/import-error"],"ename":"PydanticImportError","evalue":"`pydantic.errors:ConfigError` has been removed in V2.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/import-error","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:48:15.228030Z","iopub.execute_input":"2025-04-15T19:48:15.228626Z","iopub.status.idle":"2025-04-15T19:48:16.437299Z","shell.execute_reply.started":"2025-04-15T19:48:15.228582Z","shell.execute_reply":"2025-04-15T19:48:16.436589Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nimport torch\nimport bitsandbytes as bnb\n\n# Verify CUDA is available\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"BitsAndBytes version: {bnb.__version__}\")\n\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif torch.cuda.is_available():\n    # GPU configuration with 4-bit quantization\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True\n    )\nelse:\n    # If no GPU, try loading with 8-bit quantization or regular loading\n    print(\"No GPU detected. Loading model with basic configuration...\")\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"auto\",\n            torch_dtype=torch.float32,\n            trust_remote_code=True\n        )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        # If that fails, try with a smaller model\n        print(\"Consider using a smaller model or enabling GPU runtime\")\n        raise\n\n# Load pipeline\nqa_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:56:11.207191Z","iopub.execute_input":"2025-04-15T19:56:11.207764Z","iopub.status.idle":"2025-04-15T19:56:27.415092Z","shell.execute_reply.started":"2025-04-15T19:56:11.207723Z","shell.execute_reply":"2025-04-15T19:56:27.414213Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nCUDA version: 12.1\nBitsAndBytes version: 0.45.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50fb095c45b54f5daebb356250549023"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"def answer_clinical_query(query):\n    docs = retriever.invoke(query)\n    context = \"\\n\".join([doc.page_content for doc in docs])\n    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n    \n    response = qa_pipeline(prompt, max_new_tokens=256, num_return_sequences=1)\n    full_output = response[0]['generated_text']\n    \n    # Extract only the answer part\n    answer = full_output.split(\"Answer:\")[-1].strip()\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:12:55.423790Z","iopub.execute_input":"2025-04-15T20:12:55.424391Z","iopub.status.idle":"2025-04-15T20:12:55.433453Z","shell.execute_reply.started":"2025-04-15T20:12:55.424350Z","shell.execute_reply":"2025-04-15T20:12:55.431408Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"query = \"what is CT scan\"\nresponse = answer_clinical_query(query)\nprint(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:12:57.284061Z","iopub.execute_input":"2025-04-15T20:12:57.284425Z","iopub.status.idle":"2025-04-15T20:13:20.561343Z","shell.execute_reply.started":"2025-04-15T20:12:57.284396Z","shell.execute_reply":"2025-04-15T20:13:20.560158Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"A CT scan, or computed tomography scan, is a medical imaging test that uses X-rays and computer technology to create detailed images of the body. It can be used to diagnose a wide range of conditions, including pneumonia, cancer, and injuries. CT scans can be performed on various parts of the body, including the chest, abdomen, and head. They can also be used to guide medical procedures, such as biopsies and drainage of abscesses. CT scans are typically performed in a hospital or radiology clinic, and the results are interpreted by a radiologist.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"print(query)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:15:28.399862Z","iopub.execute_input":"2025-04-15T20:15:28.400469Z","iopub.status.idle":"2025-04-15T20:15:28.409301Z","shell.execute_reply.started":"2025-04-15T20:15:28.400413Z","shell.execute_reply":"2025-04-15T20:15:28.407573Z"}},"outputs":[{"name":"stdout","text":"what is CT scan\nA CT scan, or computed tomography scan, is a medical imaging test that uses X-rays and computer technology to create detailed images of the body. It can be used to diagnose a wide range of conditions, including pneumonia, cancer, and injuries. CT scans can be performed on various parts of the body, including the chest, abdomen, and head. They can also be used to guide medical procedures, such as biopsies and drainage of abscesses. CT scans are typically performed in a hospital or radiology clinic, and the results are interpreted by a radiologist.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Test query\nquery = \"What are the symptoms of heart failure?\"\nresponse = answer_clinical_query(query)\nprint(query)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:16:05.591445Z","iopub.execute_input":"2025-04-15T20:16:05.592092Z","iopub.status.idle":"2025-04-15T20:16:24.890222Z","shell.execute_reply.started":"2025-04-15T20:16:05.592040Z","shell.execute_reply":"2025-04-15T20:16:24.889323Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"What are the symptoms of heart failure?\nThe symptoms of heart failure include shortness of breath, fatigue, reduced exercise tolerance, ankle swelling, and weight gain. Other symptoms may include cough, wheezing, bloated feeling, loss of appetite, confusion, depression, palpitations, dizziness, syncope, and chest pain. Symptoms may vary depending on the type and severity of heart failure.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"app_code = \"\"\"\nimport streamlit as st\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nimport torch\nimport bitsandbytes as bnb\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\n# Load your embedding model and vectorstore\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n\n# Verify CUDA is available\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"BitsAndBytes version: {bnb.__version__}\")\n\n# Load tokenizer and model\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nif torch.cuda.is_available():\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True\n    )\nelse:\n    print(\"No GPU detected. Loading model with basic configuration...\")\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=\"auto\",\n            torch_dtype=torch.float32,\n            trust_remote_code=True\n        )\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\n# Load the QA pipeline\nqa_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)\n\n# Define the function to answer clinical queries\ndef answer_clinical_query(query):\n    # Get the top 5 relevant documents from the retriever\n    docs = retriever.invoke(query)\n    context = \"\\\\n\".join([doc.page_content for doc in docs])\n    \n    # Create the prompt for the language model\n    prompt = f\"Context:\\\\n{context}\\\\n\\\\nQuestion: {query}\\\\nAnswer:\"\n    \n    # Generate the response from the QA pipeline\n    response = qa_pipeline(prompt, max_new_tokens=256, num_return_sequences=1)\n    full_output = response[0]['generated_text']\n    \n    # Extract only the answer part from the response\n    answer = full_output.split(\"Answer:\")[-1].strip()\n    return answer\n\n# Streamlit app code\nst.title(\"Clinical Query Answering System\")\n\nquery = st.text_input(\"Enter your clinical query:\")\n\nif query:\n    with st.spinner(\"Processing your query...\"):\n        response = answer_clinical_query(query)\n        st.subheader(\"Answer:\")\n        st.write(response)\n\nst.markdown(\"This tool uses clinical context to answer your queries.\")\n\"\"\"\n\nwith open(\"/kaggle/working/app.py\", \"w\") as f:\n    f.write(app_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:00:09.175117Z","iopub.execute_input":"2025-04-15T21:00:09.175430Z","iopub.status.idle":"2025-04-15T21:00:09.181633Z","shell.execute_reply.started":"2025-04-15T21:00:09.175406Z","shell.execute_reply":"2025-04-15T21:00:09.180895Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"!pip install streamlit --quiet\n!streamlit run /kaggle/working/app.py & npx localtunnel --port 8501 --password mypassword123\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:00:09.755672Z","iopub.execute_input":"2025-04-15T21:00:09.756004Z","iopub.status.idle":"2025-04-15T21:04:49.548025Z","shell.execute_reply.started":"2025-04-15T21:00:09.755968Z","shell.execute_reply":"2025-04-15T21:04:49.547028Z"}},"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.41.193.90:8501\u001b[0m\n\u001b[0m\n\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0Kyour url is: https://twelve-coats-move.loca.lt\n2025-04-15 21:00:32.600625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-15 21:00:32.643005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-15 21:00:32.655242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.embeddings import HuggingFaceEmbeddings`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/__init__.py:35: LangChainDeprecationWarning: Importing vector stores from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.vectorstores import FAISS`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[94m121\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m640\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2;33m/kaggle/working/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m11\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 8 \u001b[0m                                                                                \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m# Load your embedding model and vectorstore\u001b[0m                                     \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m10 \u001b[0membedding_model = HuggingFaceEmbeddings(model_name=\u001b[33m\"\u001b[0m\u001b[33msentence-transformers/all-M\u001b[0m \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m11 vectorstore = FAISS.load_local(\u001b[33m\"\u001b[0m\u001b[33mfaiss_index\u001b[0m\u001b[33m\"\u001b[0m, embedding_model, allow_dangerous_ \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m12 \u001b[0mretriever = vectorstore.as_retriever(search_type=\u001b[33m\"\u001b[0m\u001b[33msimilarity\u001b[0m\u001b[33m\"\u001b[0m, search_kwargs={\u001b[33m\"\u001b[0m \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m13 \u001b[0m                                                                                \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m# Verify CUDA is available\u001b[0m                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/\u001b[0m\u001b[1;33mfaiss.py\u001b[0m:\u001b[94m10\u001b[0m \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[94m97\u001b[0m in \u001b[92mload_local\u001b[0m                                                                     \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m1094 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m1095 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# load docstore and index_to_docstore_id\u001b[0m                              \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m1096 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(path / \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mindex_name\u001b[33m}\u001b[0m\u001b[33m.pkl\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mrb\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mas\u001b[0m f:                     \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1097 \u001b[2m│   │   │   \u001b[0mdocstore, index_to_docstore_id = pickle.load(f)                   \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m1098 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mcls\u001b[0m(embeddings, index, docstore, index_to_docstore_id, **kwarg \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mserialize_to_bytes\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mbytes\u001b[0m:                                    \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/pydantic/v1/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m423\u001b[0m in \u001b[92m__setstate__\u001b[0m      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 420 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 421 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m__setstate__\u001b[0m(\u001b[96mself\u001b[0m, state: \u001b[33m'\u001b[0m\u001b[33mDictAny\u001b[0m\u001b[33m'\u001b[0m) -> \u001b[94mNone\u001b[0m:                         \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 422 \u001b[0m\u001b[2m│   │   \u001b[0mobject_setattr(\u001b[96mself\u001b[0m, \u001b[33m'\u001b[0m\u001b[33m__dict__\u001b[0m\u001b[33m'\u001b[0m, state[\u001b[33m'\u001b[0m\u001b[33m__dict__\u001b[0m\u001b[33m'\u001b[0m])                   \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 423 \u001b[2m│   │   \u001b[0mobject_setattr(\u001b[96mself\u001b[0m, \u001b[33m'\u001b[0m\u001b[33m__fields_set__\u001b[0m\u001b[33m'\u001b[0m, state[\u001b[33m'\u001b[0m\u001b[33m__fields_set__\u001b[0m\u001b[33m'\u001b[0m])       \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 424 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, value \u001b[95min\u001b[0m state.get(\u001b[33m'\u001b[0m\u001b[33m__private_attribute_values__\u001b[0m\u001b[33m'\u001b[0m, {}).item \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 425 \u001b[0m\u001b[2m│   │   │   \u001b[0mobject_setattr(\u001b[96mself\u001b[0m, name, value)                                 \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m 426 \u001b[0m                                                                              \u001b[31m \u001b[0m\n\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n\u001b[1;91mKeyError: \u001b[0m\u001b[32m'__fields_set__'\u001b[0m\n2025-04-15 21:00:43.072 Examining the path of torch.classes raised:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n    if asyncio.get_running_loop().is_running():\nRuntimeError: no running event loop\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n    potential_paths = extract_paths(module)\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n    lambda m: list(m.__path__._path),\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_classes.py\", line 13, in __getattr__\n    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\nRuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n^C\n\u001b[34m  Stopping...\u001b[0m\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"!curl https://loca.lt/mytunnelpassword\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:47:47.364708Z","iopub.execute_input":"2025-04-15T20:47:47.364953Z","iopub.status.idle":"2025-04-15T20:47:48.091311Z","shell.execute_reply.started":"2025-04-15T20:47:47.364931Z","shell.execute_reply":"2025-04-15T20:47:48.090137Z"}},"outputs":[{"name":"stdout","text":"34.41.193.90","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"app_code = \"\"\"\nimport streamlit as st\n\n# Dummy placeholder setup (replace with actual retriever and qa_pipeline)\nclass DummyDoc:\n    def __init__(self, page_content):\n        self.page_content = page_content\n\n# Simulate the retriever and qa_pipeline for testing\ndef retriever(query):\n    # Return a list of DummyDocs to simulate retrieved documents\n    return [DummyDoc(\"CT scan is an imaging technique that uses X-rays to create images of the body.\")]\n\ndef qa_pipeline(prompt, max_new_tokens=256, num_return_sequences=1):\n    # Simulate a response from the model\n    return [{\"generated_text\": prompt + \" A CT scan, or computed tomography scan, is a medical imaging test that uses X-rays and computer technology to create detailed images of the body. It can be used to diagnose a wide range of conditions, including pneumonia, cancer, and injuries. CT scans can be performed on various parts of the body, including the chest, abdomen, and head. They can also be used to guide medical procedures, such as biopsies and drainage of abscesses. CT scans are typically performed in a hospital or radiology clinic, and the results are interpreted by a radiologist.\"}]\n\ndef answer_clinical_query(query):\n    docs = retriever(query)\n    context = \"\\\\n\".join([doc.page_content for doc in docs])\n    prompt = f\"Context:\\\\n{context}\\\\n\\\\nQuestion: {query}\\\\nAnswer:\"\n    \n    response = qa_pipeline(prompt, max_new_tokens=256, num_return_sequences=1)\n    full_output = response[0]['generated_text']\n    \n    # Extract only the answer part\n    answer = full_output.split(\"Answer:\")[-1].strip()\n    return answer\n\nst.title(\"Clinical Query Answering System\")\n\nquery = st.text_input(\"Enter your clinical query:\")\n\nif query:\n    with st.spinner(\"Processing your query...\"):\n        response = answer_clinical_query(query)\n        st.subheader(\"Answer:\")\n        st.write(response)\n\nst.markdown(\"This tool uses clinical context to answer your queries.\")\n\"\"\"\n\nwith open(\"/kaggle/working/app.py\", \"w\") as f:\n    f.write(app_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:07:22.117443Z","iopub.execute_input":"2025-04-15T21:07:22.117856Z","iopub.status.idle":"2025-04-15T21:07:22.125179Z","shell.execute_reply.started":"2025-04-15T21:07:22.117817Z","shell.execute_reply":"2025-04-15T21:07:22.123844Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"!pip install streamlit --quiet\n!streamlit run /kaggle/working/app.py & npx localtunnel --port 8501 --password mypassword123\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:07:23.232214Z","iopub.execute_input":"2025-04-15T21:07:23.233253Z"}},"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.41.193.90:8501\u001b[0m\n\u001b[0m\n\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://wide-rooms-take.loca.lt\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}