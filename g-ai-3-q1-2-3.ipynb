{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10860736,"sourceType":"datasetVersion","datasetId":6746731},{"sourceId":269826,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":230911,"modelId":252668}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ROLL No:** 21F-9266,21F-9224","metadata":{}},{"cell_type":"markdown","source":"# ***MODEL WITH SOME OUTPUT***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport time\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Set the environment variable to avoid fragmentation\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Load the dataset (replace 'spoc-train.tsv' with your actual file path)\ndata = pd.read_csv('/kaggle/input/spoc-data/spoc-train.tsv', sep='\\t')\n\n# Keep only 'text' and 'code' columns\ndata = data[['text', 'code']]\n\n# Check for missing values and drop them if any\ndata = data.dropna()\n\n# Split into train (80%) and temp (20%)\ntrain_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Split temp into validation (10%) and test (10%)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Validation samples: {len(val_data)}\")\nprint(f\"Test samples: {len(test_data)}\")\n\n# Simple tokenizer (splits on spaces)\ndef tokenize(text):\n    return text.split()\n\n# Build vocabularies from training data\ndef build_vocab(data, tokenizer):\n    tokens = set()\n    for item in data:\n        tokens.update(tokenizer(item))\n    # Reserve 0 for padding, 1 for <sos>, 2 for <eos>\n    vocab = {token: idx for idx, token in enumerate(tokens, start=3)}\n    vocab['<pad>'] = 0\n    vocab['<sos>'] = 1\n    vocab['<eos>'] = 2\n    return vocab\n\n# Create vocabularies\npseudocode_vocab = build_vocab(train_data['text'], tokenize)\ncpp_vocab = build_vocab(train_data['code'], tokenize)\n\n# Inverse vocabularies for decoding (optional)\ninv_cpp_vocab = {idx: token for token, idx in cpp_vocab.items()}\n\n# Create Dataset class\nclass CodeDataset(Dataset):\n    def __init__(self, data, pseudocode_vocab, cpp_vocab, tokenizer):\n        self.data = data\n        self.pseudocode_vocab = pseudocode_vocab\n        self.cpp_vocab = cpp_vocab\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        pseudocode = self.data.iloc[idx]['text']\n        cpp = self.data.iloc[idx]['code']\n        # Add special tokens\n        pseudo_tokens = ['<sos>'] + self.tokenizer(pseudocode) + ['<eos>']\n        cpp_tokens = ['<sos>'] + self.tokenizer(cpp) + ['<eos>']\n        # Convert to indices\n        pseudo_indices = [self.pseudocode_vocab.get(token, 0) for token in pseudo_tokens]\n        cpp_indices = [self.cpp_vocab.get(token, 0) for token in cpp_tokens]\n        return torch.tensor(pseudo_indices), torch.tensor(cpp_indices)\n\n# Padding function for batches\ndef collate_fn(batch):\n    pseudocode, cpp = zip(*batch)\n    pseudocode = torch.nn.utils.rnn.pad_sequence(pseudocode, padding_value=0, batch_first=True)\n    cpp = torch.nn.utils.rnn.pad_sequence(cpp, padding_value=0, batch_first=True)\n    return pseudocode, cpp\n\n# Create DataLoaders\nbatch_size = 8  # Reduced batch size due to memory constraints\ntrain_dataset = CodeDataset(train_data, pseudocode_vocab, cpp_vocab, tokenize)\nval_dataset = CodeDataset(val_data, pseudocode_vocab, cpp_vocab, tokenize)\ntest_dataset = CodeDataset(test_data, pseudocode_vocab, cpp_vocab, tokenize)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n\n# Multi-Head Attention\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        d_k = Q.size(-1)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn = torch.softmax(scores, dim=-1)\n        return torch.matmul(attn, V), attn\n\n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        output, attn = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        return self.W_o(output)\n\n\n# Feed-Forward Network\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n\n\n# Encoder Layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        x = self.norm1(x + self.dropout(self.mha(x, x, x, mask)))\n        x = self.norm2(x + self.dropout(self.ff(x)))\n        return x\n\n\n# Decoder Layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        x = self.norm1(x + self.dropout(self.mha1(x, x, x, tgt_mask)))\n        x = self.norm2(x + self.dropout(self.mha2(x, enc_output, enc_output, src_mask)))\n        x = self.norm3(x + self.dropout(self.ff(x)))\n        return x\n\n\n# Transformer Model\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, num_heads=2, num_layers=2, d_ff=256, dropout=0.1, max_len=512):\n        super().__init__()\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.d_model = d_model\n\n    def create_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # Padding mask\n        tgt_seq_len = tgt.size(1)\n        nopeak_mask = torch.tril(torch.ones(tgt_seq_len, tgt_seq_len)).bool().to(tgt.device)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2) & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.create_mask(src, tgt)\n        src_embedded = self.dropout(self.pos_encoding(self.src_embedding(src) * math.sqrt(self.d_model)))\n        tgt_embedded = self.dropout(self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))\n        \n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n        \n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        return self.fc_out(dec_output)\n\n# Initialize the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Transformer(\n    src_vocab_size=len(pseudocode_vocab),\n    tgt_vocab_size=len(cpp_vocab),\n    d_model=64,  # Further reduced hidden units for memory efficiency\n    num_heads=2,  # Reduced number of attention heads\n    num_layers=2,  # Reduced number of layers\n    d_ff=128,  # Reduced feed-forward dimension\n    dropout=0.1\n).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\noptimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:03:39.198741Z","iopub.execute_input":"2025-02-27T08:03:39.199141Z","iopub.status.idle":"2025-02-27T08:03:40.014918Z","shell.execute_reply.started":"2025-02-27T08:03:39.199117Z","shell.execute_reply":"2025-02-27T08:03:40.014014Z"}},"outputs":[{"name":"stdout","text":"Training samples: 172980\nValidation samples: 21622\nTest samples: 21623\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# Training loop without mixed precision\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    start_time = time.time()\n    train_loss = 0\n    \n    for pseudocode, cpp in train_loader:\n        pseudocode, cpp = pseudocode.to(device), cpp.to(device)\n        optimizer.zero_grad()\n        \n        # Decoder input: <sos> + target[:-1]\n        # Target output: target[1:] + <eos>\n        output = model(pseudocode, cpp[:, :-1])\n        loss = criterion(output.view(-1, len(cpp_vocab)), cpp[:, 1:].contiguous().view(-1))\n\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n    \n    train_loss /= len(train_loader)\n    end_time = time.time()\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for pseudocode, cpp in val_loader:\n            pseudocode, cpp = pseudocode.to(device), cpp.to(device)\n            output = model(pseudocode, cpp[:, :-1])\n            loss = criterion(output.view(-1, len(cpp_vocab)), cpp[:, 1:].contiguous().view(-1))\n            val_loss += loss.item()\n    val_loss /= len(val_loader)\n\n    # Display results\n    epoch_time = end_time - start_time\n    print(f\"Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.2f}s, Val Loss: {val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:03:49.683201Z","iopub.execute_input":"2025-02-27T08:03:49.683521Z","iopub.status.idle":"2025-02-27T08:35:27.572990Z","shell.execute_reply.started":"2025-02-27T08:03:49.683496Z","shell.execute_reply":"2025-02-27T08:35:27.572003Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Time: 363.58s, Val Loss: 2.1023\nEpoch 2/5, Time: 360.98s, Val Loss: 1.8708\nEpoch 3/5, Time: 360.65s, Val Loss: 1.7683\nEpoch 4/5, Time: 360.75s, Val Loss: 1.7206\nEpoch 5/5, Time: 360.35s, Val Loss: 1.8546\nTest Loss: 1.8228\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Optional: Evaluate on test set\nmodel.eval()\ntest_loss = 0\nwith torch.no_grad():\n    for pseudocode, cpp in test_loader:\n        pseudocode, cpp = pseudocode.to(device), cpp.to(device)\n        output = model(pseudocode, cpp[:, :-1])\n        loss = criterion(output.view(-1, len(cpp_vocab)), cpp[:, 1:].contiguous().view(-1))\n        test_loss += loss.item()\ntest_loss /= len(test_loader)\nprint(f\"Test Loss: {test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:49:19.164659Z","iopub.execute_input":"2025-02-27T08:49:19.165053Z","iopub.status.idle":"2025-02-27T08:49:34.428199Z","shell.execute_reply.started":"2025-02-27T08:49:19.165026Z","shell.execute_reply":"2025-02-27T08:49:34.427285Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 1.8228\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"torch.save(model.state_dict(), \"transformer_seq2seq_testing0.2.pth\")\nprint(\"Model saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:55:02.903095Z","iopub.execute_input":"2025-02-27T08:55:02.903458Z","iopub.status.idle":"2025-02-27T08:55:02.955105Z","shell.execute_reply.started":"2025-02-27T08:55:02.903433Z","shell.execute_reply":"2025-02-27T08:55:02.954408Z"}},"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Load model\nmodel.load_state_dict(torch.load(\"/kaggle/input/test0.1/pytorch/test0.1/1/transformer_seq2seq_testing0.1.pth\"))\nmodel.eval()\nprint(\"Model loaded successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inverse vocabularies for decoding (for C++ code)\ninv_code_vocab = {idx: token for token, idx in cpp_vocab.items()}\n\ndef generate_code(model, input_text, text_vocab, code_vocab, max_len=100):\n    \"\"\"\n    Generate C++ code from pseudocode using the trained Transformer model.\n    \n    :param model: The trained Transformer model.\n    :param input_text: The input pseudocode as a string.\n    :param text_vocab: Vocabulary for pseudocode (text).\n    :param code_vocab: Vocabulary for C++ code.\n    :param max_len: Maximum length of the generated code.\n    :return: The generated C++ code as a string.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    \n    # Tokenize the input text (pseudocode)\n    input_tokens = ['<sos>'] + input_text.split() + ['<eos>']\n    \n    # Convert tokens to indices using the text vocabulary\n    input_indices = [text_vocab.get(token, 0) for token in input_tokens]\n    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)  # Add batch dimension\n    \n    # Start decoding the output C++ code\n    generated_code = []\n    tgt_input = torch.tensor([text_vocab['<sos>']]).unsqueeze(0).to(device)  # Initial target token\n    \n    for _ in range(max_len):\n        with torch.no_grad():\n            # Get the model's prediction for the next token\n            output = model(input_tensor, tgt_input)\n            \n            # Get the predicted token (with highest probability) for each position\n            predicted_token_idx = output.argmax(dim=-1)[:, -1].item()  # Get the last token prediction\n            \n            # If we predict the <eos> token, stop generating\n            if predicted_token_idx == code_vocab['<eos>']:\n                break\n            \n            # Add the predicted token to the output sequence\n            generated_code.append(inv_code_vocab.get(predicted_token_idx, '<unk>'))\n            \n            # Update the target input (append the predicted token for the next step)\n            tgt_input = torch.cat([tgt_input, torch.tensor([[predicted_token_idx]]).to(device)], dim=-1)\n    \n    # Join the generated tokens into a single string\n    generated_code_str = ' '.join(generated_code)\n    return generated_code_str\n\n# Example usage:\ninput_text = \"for i from 1 to n do\"\ngenerated_code = generate_code(model, input_text, pseudocode_vocab, cpp_vocab, max_len=100)\nprint(\"Generated C++ code:\", generated_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:55:08.520910Z","iopub.execute_input":"2025-02-27T08:55:08.521233Z","iopub.status.idle":"2025-02-27T08:55:08.587238Z","shell.execute_reply.started":"2025-02-27T08:55:08.521205Z","shell.execute_reply":"2025-02-27T08:55:08.586499Z"}},"outputs":[{"name":"stdout","text":"Generated C++ code: for (int i = 1; i <= n; i++) {\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"input_text = \"create integers x1, y1, x2, y2\"\ngenerated_code = generate_code(model, input_text, pseudocode_vocab, cpp_vocab, max_len=100)\nprint(\"Generated C++ code:\", generated_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:55:16.738889Z","iopub.execute_input":"2025-02-27T08:55:16.739183Z","iopub.status.idle":"2025-02-27T08:55:16.767955Z","shell.execute_reply.started":"2025-02-27T08:55:16.739163Z","shell.execute_reply":"2025-02-27T08:55:16.767213Z"}},"outputs":[{"name":"stdout","text":"Generated C++ code: int n, m, k;\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"input_text = \"read s\"\ngenerated_code = generate_code(model, input_text, pseudocode_vocab, cpp_vocab, max_len=100)\nprint(\"Generated C++ code:\", generated_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:55:29.588059Z","iopub.execute_input":"2025-02-27T08:55:29.588333Z","iopub.status.idle":"2025-02-27T08:55:29.611678Z","shell.execute_reply.started":"2025-02-27T08:55:29.588313Z","shell.execute_reply":"2025-02-27T08:55:29.610869Z"}},"outputs":[{"name":"stdout","text":"Generated C++ code: cin >> s;\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import gradio as gr\n\n# Define the function that uses the generate_code logic\ndef generate_cpp_code_from_pseudocode(input_text):\n    generated_code = generate_code(model, input_text, pseudocode_vocab, cpp_vocab, max_len=100)\n    return generated_code\n\n# Create a Gradio interface\ninterface = gr.Interface(\n    fn=generate_cpp_code_from_pseudocode,          # Function to run\n    inputs=gr.Textbox(lines=2, placeholder=\"Enter pseudocode here...\"),  # Input text box for pseudocode\n    outputs=gr.Textbox(label=\"Generated C++ Code\"),  # Output text box for generated C++ code\n    title=\"Pseudocode to C++ Code Generator\",  # Title of the interface\n    description=\"Enter pseudocode and get the corresponding C++ code generated using the Transformer model.\",\n    theme=\"compact\"\n)\n\n# Launch the interface\ninterface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:56:53.988040Z","iopub.execute_input":"2025-02-27T08:56:53.988366Z","iopub.status.idle":"2025-02-27T08:56:59.417088Z","shell.execute_reply.started":"2025-02-27T08:56:53.988342Z","shell.execute_reply":"2025-02-27T08:56:59.416304Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1096: UserWarning: Cannot load compact. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/compact (Request ID: Root=1-67c028d9-28cbc996374980e966882819;ad245594-c15e-4f89-8367-2abbc13e4ddd)\n\nSorry, we can't find the page you are looking for.\n  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://57ed0cbbb7bf79fefa.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://57ed0cbbb7bf79fefa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:55:47.636774Z","iopub.execute_input":"2025-02-27T08:55:47.637129Z","iopub.status.idle":"2025-02-27T08:55:58.492149Z","shell.execute_reply.started":"2025-02-27T08:55:47.637101Z","shell.execute_reply":"2025-02-27T08:55:58.491213Z"}},"outputs":[{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-5.19.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.2 (from gradio)\n  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a2)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio) (2024.12.0)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.29.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.19.0-py3-none-any.whl (62.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\nSuccessfully installed fastapi-0.115.8 ffmpy-0.5.0 gradio-5.19.0 gradio-client-1.7.2 markupsafe-2.1.5 python-multipart-0.0.20 ruff-0.9.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}